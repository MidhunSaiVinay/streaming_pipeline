#!/usr/bin/env bash
set -euo pipefail

MINUTES="${1:-5}"
echo "â–¶ï¸  Launching producer (background)â€¦"
docker compose run -d --name producer_run --rm producer

echo "ğŸ”¥ Running Spark job for ${MINUTES} minute(s)â€¦"
docker compose run --rm spark spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  /opt/jobs/spark_stream.py \
  --bootstrap kafka:9092 \
  --topic user_events \
  --out /opt/datalake/events \
  --checkpoint /opt/checkpoints/user_events \
  --minutes "${MINUTES}"

echo "ğŸ§¹ Stopping producerâ€¦"
docker stop producer_run >/dev/null || true

echo "ğŸ“¦ Output written under ./datalake/events (partitioned Parquet)."
